{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Tune the weights of the weighted functions used in SynBA transformation (`WordSwapMultimodal`).\n",
    "\n",
    "To do so, use [HyperOpt](https://github.com/hyperopt/hyperopt) library.\n",
    "TPE is a default algorithm for the Hyperopt. It uses Bayesian approach for optimization. At every step it is trying to build probabilistic model of the function and choose the most promising parameters for the next step. Generally this types of algorithms works like this:\n",
    "\n",
    "1. Generate random initial point  $x^∗$ \n",
    "2. Calculate  $F(x^∗)$\n",
    "3. Using the history of trials try to build the conditional probability model  $P(F|x)$ \n",
    "4. Choose  $x_i$  that according to the  $P(F|x)$  will most probably result in better  $F(x_i)$\n",
    "5. Compute the real value of the  $F(x_i)$ \n",
    "6. Repeat steps 3-5 until one of the stop criteria is satisfied, for example $i > max_eval$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peppe\\anaconda3\\envs\\3rdplace\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import transformers\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error() # disable transformers logging\n",
    "\n",
    "from textattack.attack_recipes import SynBA2022\n",
    "\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "\n",
    "from textattack import Attack\n",
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.constraints.pre_transformation import (\n",
    "    MaxModificationRate,\n",
    "    RepeatModification,\n",
    "    StopwordModification,\n",
    "    InputColumnModification\n",
    ")\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.constraints.semantics.sentence_encoders import BERT\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "from textattack.transformations import WordSwapMultimodal\n",
    "from textattack import Attacker\n",
    "from textattack import AttackArgs\n",
    "\n",
    "from textattack.metrics.quality_metrics import (\n",
    "    SBERTMetric,\n",
    "    ContradictionMetric\n",
    ")\n",
    "from textattack.metrics.attack_metrics import (\n",
    "    AttackSuccessRate\n",
    ")\n",
    "\n",
    "# Import HyperOpt Library\n",
    "from hyperopt import tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (C:\\Users\\peppe\\.cache\\huggingface\\datasets\\rotten_tomatoes_movie_review\\default\\1.0.0\\e06abb624abab47e1a64608fdfe65a913f5a68c66118408032644a3285208fb5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 500.04it/s]\n",
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtest\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "# Import the model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-rotten-tomatoes\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-rotten-tomatoes\")\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Import the dataset\n",
    "dataset_name = \"rotten_tomatoes\"\n",
    "dataset = HuggingFaceDataset(dataset_name, None, \"test\")\n",
    "\n",
    "# whether to make the attack reproducible or set it random for each run\n",
    "random_samples = False\n",
    "random_seed = random.randint(0, 10000) if random_samples else 445\n",
    "\n",
    "# set the number of samples to attack\n",
    "num_examples=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_synba(model_wrapper, lambda_mlm, lambda_thesaurus, lambda_we):\n",
    "    \"\"\"\n",
    "    Build the attack, with the given hyperparameters\n",
    "    \"\"\"\n",
    "    #\n",
    "    # Candidate size K is set to 30 for all data-sets.\n",
    "    transformation = WordSwapMultimodal(max_candidates=30, multimodal_weights=(lambda_mlm, lambda_thesaurus, lambda_we), min_confidence=0.1)\n",
    "    #\n",
    "    # Don't modify the same word twice or stopwords.\n",
    "    #\n",
    "    constraints = [RepeatModification(), StopwordModification()]\n",
    "    input_column_modification = InputColumnModification(\n",
    "        [\"premise\", \"hypothesis\"], {\"premise\"}\n",
    "    )\n",
    "    constraints.append(input_column_modification)\n",
    "    constraints.append(MaxModificationRate(max_rate=0.3, min_threshold=4))\n",
    "    constraints.append(PartOfSpeech(allow_verb_noun_swap=False))\n",
    "    constraints.append(WordEmbeddingDistance(min_cos_sim=0.6, include_unknown_words=True))\n",
    "    constraints.append(BERT(model_name=\"stsb-mpnet-base-v2\", threshold=0.7, metric=\"cosine\"))\n",
    "    #\n",
    "    # Goal is untargeted classification\n",
    "    #\n",
    "    goal_function = UntargetedClassification(model_wrapper)\n",
    "    #\n",
    "    # Greedily swap words with \"Word Importance Ranking\".\n",
    "    #\n",
    "    search_method = GreedyWordSwapWIR(wir_method=\"gradient\")\n",
    "\n",
    "    return Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "def perform_attack(model, dataset, num_examples, random_seed, lambda_mlm, lambda_thesaurus, lambda_we):\n",
    "    \"\"\"\n",
    "    Set up the attack and perform it on the dataset.\n",
    "    Returns the metrics that we want to optimize.\n",
    "    \"\"\"\n",
    "    attack = build_synba(model, lambda_mlm, lambda_thesaurus, lambda_we)\n",
    "    attack_args = AttackArgs(num_examples=num_examples, shuffle=False, random_seed=random_seed, disable_stdout=True, silent=True)\n",
    "    attacker = Attacker(attack, dataset, attack_args)\n",
    "    attack_results = attacker.attack_dataset()\n",
    "\n",
    "    # compute metrics\n",
    "    attack_success_stats = AttackSuccessRate().calculate(attack_results)\n",
    "    sbert_stats = SBERTMetric().calculate(attack_results)[\"avg_attack_sentence_bert_similarity\"]\n",
    "    contradiction_stats = ContradictionMetric(by_sentence=True).calculate(attack_results)[\"attack_contradiction_rate\"]\n",
    "\n",
    "    # free memory after performing the attack\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return attack_success_stats, sbert_stats, contradiction_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(args):\n",
    "    \"\"\"\n",
    "    This is the function that hyperopt will maximize.\n",
    "    \"\"\"\n",
    "    # make sure that the hyperparameters sum up to 1\n",
    "    w1 = args[\"w1\"]\n",
    "    w2 = (1 - args[\"w1\"]) * args[\"w2\"]\n",
    "    w3 = (1 - args[\"w1\"]) * (1 - args[\"w2\"])\n",
    "\n",
    "    print(f\"Now tring --> lambda_mlm: {round(w1, 4)}, lambda_thesaurus: {round(w2, 4)}, lambda_we: {round(w3, 4)}\")\n",
    "    attack_success_stats, sbert_similarity, contraddiction_rate = perform_attack(model_wrapper, dataset, num_examples, random_seed, w1, w2, w3)\n",
    "    \n",
    "    print(f\"Succ/Fail/Skip: {attack_success_stats['successful_attacks']}/{attack_success_stats['failed_attacks']}/{attack_success_stats['skipped_attacks']}\")\n",
    "    print(f\"--> SBERT Similarity: {sbert_similarity}, Contraddiction Rate: {contraddiction_rate}\")\n",
    "    \n",
    "    # compute penality for failed attacks\n",
    "    penalty = 0.2 * (attack_success_stats[\"failed_attacks\"] / (attack_success_stats[\"failed_attacks\"]+attack_success_stats[\"successful_attacks\"]))\n",
    "\n",
    "    # objective to maximize (minimize the negative)\n",
    "    loss = -(sbert_similarity * (1-contraddiction_rate)) + penalty\n",
    "    print(f\"--> Loss: {loss}\\n\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tring --> lambda_mlm: 0.622, lambda_thesaurus: 0.0763, lambda_we: 0.3018\n",
      "Succ/Fail/Skip: 323/83/94                              \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.139\n",
      "--> Loss: -0.7331523004926108                          \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2177, lambda_thesaurus: 0.1074, lambda_we: 0.6749          \n",
      "Succ/Fail/Skip: 327/79/94                                                              \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.147                                \n",
      "--> Loss: -0.7262247438423645                                                          \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9883, lambda_thesaurus: 0.0028, lambda_we: 0.0088          \n",
      "Succ/Fail/Skip: 16/390/94                                                              \n",
      "--> SBERT Similarity: 0.966, Contraddiction Rate: 0.062                                \n",
      "--> Loss: -0.7139897733990147                                                          \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9365, lambda_thesaurus: 0.022, lambda_we: 0.0415           \n",
      "Succ/Fail/Skip: 18/388/94                                                              \n",
      "--> SBERT Similarity: 0.965, Contraddiction Rate: 0.056                                  \n",
      "--> Loss: -0.7198269950738915                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0552, lambda_thesaurus: 0.7324, lambda_we: 0.2124            \n",
      "Succ/Fail/Skip: 305/101/94                                                              \n",
      "--> SBERT Similarity: 0.89, Contraddiction Rate: 0.157                                  \n",
      "--> Loss: -0.7005163054187192                                                           \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5887, lambda_thesaurus: 0.3027, lambda_we: 0.1086            \n",
      "Succ/Fail/Skip: 243/163/94                                                               \n",
      "--> SBERT Similarity: 0.887, Contraddiction Rate: 0.091                                  \n",
      "--> Loss: -0.725987433497537                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.147, lambda_thesaurus: 0.0167, lambda_we: 0.8363             \n",
      "Succ/Fail/Skip: 322/84/94                                                               \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.143                                 \n",
      "--> Loss: -0.7290636896551724                                                           \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5833, lambda_thesaurus: 0.0739, lambda_we: 0.3427            \n",
      "Succ/Fail/Skip: 325/81/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.142                                  \n",
      "--> Loss: -0.7305825221674878                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4789, lambda_thesaurus: 0.0775, lambda_we: 0.4436            \n",
      "Succ/Fail/Skip: 326/80/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.141                                  \n",
      "--> Loss: -0.7319731330049262                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1496, lambda_thesaurus: 0.7926, lambda_we: 0.0579            \n",
      "Succ/Fail/Skip: 225/181/94                                                               \n",
      "--> SBERT Similarity: 0.881, Contraddiction Rate: 0.12                                   \n",
      "--> Loss: -0.6861174384236453                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2571, lambda_thesaurus: 0.2332, lambda_we: 0.5097             \n",
      "Succ/Fail/Skip: 329/77/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.137                                   \n",
      "--> Loss: -0.7361799655172414                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0385, lambda_thesaurus: 0.5289, lambda_we: 0.4327             \n",
      "Succ/Fail/Skip: 319/87/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.147                                   \n",
      "--> Loss: -0.7197248571428572                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7451, lambda_thesaurus: 0.109, lambda_we: 0.1458              \n",
      "Succ/Fail/Skip: 297/109/94                                                                \n",
      "--> SBERT Similarity: 0.892, Contraddiction Rate: 0.158                                   \n",
      "--> Loss: -0.6973694187192118                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.439, lambda_thesaurus: 0.0163, lambda_we: 0.5447              \n",
      "Succ/Fail/Skip: 324/82/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.148                                   \n",
      "--> Loss: -0.7247019113300492                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3073, lambda_thesaurus: 0.3942, lambda_we: 0.2985             \n",
      "Succ/Fail/Skip: 313/93/94                                                                 \n",
      "--> SBERT Similarity: 0.895, Contraddiction Rate: 0.144                                   \n",
      "--> Loss: -0.7203071921182266                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3797, lambda_thesaurus: 0.1039, lambda_we: 0.5164             \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.141                                   \n",
      "--> Loss: -0.7319731330049262                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0221, lambda_thesaurus: 0.5572, lambda_we: 0.4207             \n",
      "Succ/Fail/Skip: 318/88/94                                                                 \n",
      "--> SBERT Similarity: 0.893, Contraddiction Rate: 0.148                                   \n",
      "--> Loss: -0.7174862463054187                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5914, lambda_thesaurus: 0.0721, lambda_we: 0.3366             \n",
      "Succ/Fail/Skip: 325/81/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.142                                   \n",
      "--> Loss: -0.7305825221674878                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.034, lambda_thesaurus: 0.3455, lambda_we: 0.6205              \n",
      "Succ/Fail/Skip: 325/81/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.138                                   \n",
      "--> Loss: -0.7341745221674877                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1804, lambda_thesaurus: 0.3199, lambda_we: 0.4997             \n",
      "Succ/Fail/Skip: 328/78/94                                                                 \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.143                                   \n",
      "--> Loss: -0.729448354679803                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2704, lambda_thesaurus: 0.2072, lambda_we: 0.5224             \n",
      "Succ/Fail/Skip: 328/78/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.14                                    \n",
      "--> Loss: -0.732996354679803                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3493, lambda_thesaurus: 0.3091, lambda_we: 0.3416             \n",
      "Succ/Fail/Skip: 318/88/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.151                                   \n",
      "--> Loss: -0.7156562463054187                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0875, lambda_thesaurus: 0.6143, lambda_we: 0.2982             \n",
      "Succ/Fail/Skip: 304/102/94                                                                \n",
      "--> SBERT Similarity: 0.889, Contraddiction Rate: 0.155                                   \n",
      "--> Loss: -0.7009586945812808                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2388, lambda_thesaurus: 0.2439, lambda_we: 0.5173             \n",
      "Succ/Fail/Skip: 328/78/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.137                                   \n",
      "--> Loss: -0.735687354679803                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2518, lambda_thesaurus: 0.0556, lambda_we: 0.6926             \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.15                                    \n",
      "--> Loss: -0.723891133004926                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3755, lambda_thesaurus: 0.1874, lambda_we: 0.4371             \n",
      "Succ/Fail/Skip: 327/79/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.135                                   \n",
      "--> Loss: -0.7378537438423645                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7298, lambda_thesaurus: 0.1219, lambda_we: 0.1484             \n",
      "Succ/Fail/Skip: 297/109/94                                                                \n",
      "--> SBERT Similarity: 0.892, Contraddiction Rate: 0.152                                   \n",
      "--> Loss: -0.7027214187192118                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4421, lambda_thesaurus: 0.1408, lambda_we: 0.417              \n",
      "Succ/Fail/Skip: 325/81/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.138                                   \n",
      "--> Loss: -0.7341745221674877                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3773, lambda_thesaurus: 0.6132, lambda_we: 0.0095             \n",
      "Succ/Fail/Skip: 222/184/94                                                                \n",
      "--> SBERT Similarity: 0.881, Contraddiction Rate: 0.131                                   \n",
      "--> Loss: -0.67494860591133                                                               \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7201, lambda_thesaurus: 0.1476, lambda_we: 0.1323             \n",
      "Succ/Fail/Skip: 266/140/94                                                                \n",
      "--> SBERT Similarity: 0.901, Contraddiction Rate: 0.139                                   \n",
      "--> Loss: -0.7067954827586207                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5432, lambda_thesaurus: 0.2801, lambda_we: 0.1767             \n",
      "Succ/Fail/Skip: 295/111/94                                                                \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.149                                   \n",
      "--> Loss: -0.707816197044335                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6538, lambda_thesaurus: 0.0284, lambda_we: 0.3179             \n",
      "Succ/Fail/Skip: 325/81/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.138                                   \n",
      "--> Loss: -0.7341745221674877                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8689, lambda_thesaurus: 0.0307, lambda_we: 0.1004             \n",
      "Succ/Fail/Skip: 127/279/94                                                                \n",
      "--> SBERT Similarity: 0.91, Contraddiction Rate: 0.071                                    \n",
      "--> Loss: -0.7079515763546799                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3508, lambda_thesaurus: 0.5484, lambda_we: 0.1009             \n",
      "Succ/Fail/Skip: 231/175/94                                                                \n",
      "--> SBERT Similarity: 0.879, Contraddiction Rate: 0.113                                   \n",
      "--> Loss: -0.6934661034482759                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4864, lambda_thesaurus: 0.1587, lambda_we: 0.355              \n",
      "Succ/Fail/Skip: 327/79/94                                                                 \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.138                                    \n",
      "--> Loss: -0.7334357438423645                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1141, lambda_thesaurus: 0.3546, lambda_we: 0.5313              \n",
      "Succ/Fail/Skip: 324/82/94                                                                  \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.139                                    \n",
      "--> Loss: -0.7319229113300493                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1911, lambda_thesaurus: 0.0825, lambda_we: 0.7264              \n",
      "Succ/Fail/Skip: 326/80/94                                                                  \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.144                                    \n",
      "--> Loss: -0.7292791330049262                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4222, lambda_thesaurus: 0.3723, lambda_we: 0.2054              \n",
      "Succ/Fail/Skip: 292/114/94                                                                 \n",
      "--> SBERT Similarity: 0.893, Contraddiction Rate: 0.164                                    \n",
      "--> Loss: -0.6903903645320197                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2968, lambda_thesaurus: 0.1589, lambda_we: 0.5443              \n",
      "Succ/Fail/Skip: 327/79/94                                                                  \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.141                                    \n",
      "--> Loss: -0.7324657438423645                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6609, lambda_thesaurus: 0.1685, lambda_we: 0.1706              \n",
      "Succ/Fail/Skip: 307/99/94                                                                  \n",
      "--> SBERT Similarity: 0.892, Contraddiction Rate: 0.15                                     \n",
      "--> Loss: -0.709431527093596                                                               \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5304, lambda_thesaurus: 0.3518, lambda_we: 0.1178              \n",
      "Succ/Fail/Skip: 251/155/94                                                                 \n",
      "--> SBERT Similarity: 0.891, Contraddiction Rate: 0.092                                    \n",
      "--> Loss: -0.7326733201970445                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.823, lambda_thesaurus: 0.0487, lambda_we: 0.1283               \n",
      "Succ/Fail/Skip: 249/157/94                                                                 \n",
      "--> SBERT Similarity: 0.902, Contraddiction Rate: 0.108                                    \n",
      "--> Loss: -0.7272440985221675                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1978, lambda_thesaurus: 0.0989, lambda_we: 0.7033              \n",
      "Succ/Fail/Skip: 326/80/94                                                                  \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.144                                    \n",
      "--> Loss: -0.7292791330049262                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0935, lambda_thesaurus: 0.0052, lambda_we: 0.9013              \n",
      "Succ/Fail/Skip: 321/85/94                                                                  \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.143                                    \n",
      "--> Loss: -0.726857078817734                                                               \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3019, lambda_thesaurus: 0.2604, lambda_we: 0.4377              \n",
      "Succ/Fail/Skip: 328/78/94                                                                  \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.131                                    \n",
      "--> Loss: -0.742807354679803                                                               \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5266, lambda_thesaurus: 0.1676, lambda_we: 0.3059              \n",
      "Succ/Fail/Skip: 327/79/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.135                                   \n",
      "--> Loss: -0.7369887438423645                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3223, lambda_thesaurus: 0.5718, lambda_we: 0.1059             \n",
      "Succ/Fail/Skip: 243/163/94                                                                \n",
      "--> SBERT Similarity: 0.882, Contraddiction Rate: 0.099                                   \n",
      "--> Loss: -0.7143864334975369                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9723, lambda_thesaurus: 0.0118, lambda_we: 0.0159             \n",
      "Succ/Fail/Skip: 16/390/94                                                                 \n",
      "--> SBERT Similarity: 0.972, Contraddiction Rate: 0.062                                   \n",
      "--> Loss: -0.7196177733990147                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4206, lambda_thesaurus: 0.1211, lambda_we: 0.4583             \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.138                                   \n",
      "--> Loss: -0.7346671330049261                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4759, lambda_thesaurus: 0.0218, lambda_we: 0.5023             \n",
      "Succ/Fail/Skip: 324/82/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.145                                   \n",
      "--> Loss: -0.7273959113300492                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1518, lambda_thesaurus: 0.5831, lambda_we: 0.2651             \n",
      "Succ/Fail/Skip: 304/102/94                                                                \n",
      "--> SBERT Similarity: 0.891, Contraddiction Rate: 0.158                                   \n",
      "--> Loss: -0.6999756945812807                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5733, lambda_thesaurus: 0.2275, lambda_we: 0.1992             \n",
      "Succ/Fail/Skip: 308/98/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.153                                   \n",
      "--> Loss: -0.7089421379310344                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.637, lambda_thesaurus: 0.1466, lambda_we: 0.2164              \n",
      "Succ/Fail/Skip: 317/89/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.142                                   \n",
      "--> Loss: -0.7257836354679803                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.0027, lambda_thesaurus: 0.5844, lambda_we: 0.4129             \n",
      "Succ/Fail/Skip: 316/90/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.146                                   \n",
      "--> Loss: -0.7191410246305419                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3896, lambda_thesaurus: 0.0854, lambda_we: 0.525              \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.141                                   \n",
      "--> Loss: -0.7319731330049262                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2744, lambda_thesaurus: 0.1361, lambda_we: 0.5894             \n",
      "Succ/Fail/Skip: 327/79/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.147                                   \n",
      "--> Loss: -0.7262247438423645                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2185, lambda_thesaurus: 0.3633, lambda_we: 0.4183             \n",
      "Succ/Fail/Skip: 323/83/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.158                                   \n",
      "--> Loss: -0.7118613004926108                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7777, lambda_thesaurus: 0.0728, lambda_we: 0.1496             \n",
      "Succ/Fail/Skip: 302/104/94                                                                \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.146                                   \n",
      "--> Loss: -0.712244472906404                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6904, lambda_thesaurus: 0.2579, lambda_we: 0.0517             \n",
      "Succ/Fail/Skip: 223/183/94                                                                \n",
      "--> SBERT Similarity: 0.883, Contraddiction Rate: 0.13                                    \n",
      "--> Loss: -0.6780622167487684                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.057, lambda_thesaurus: 0.35, lambda_we: 0.593                 \n",
      "Succ/Fail/Skip: 325/81/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.142                                   \n",
      "--> Loss: -0.7297245221674877                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.339, lambda_thesaurus: 0.3235, lambda_we: 0.3375              \n",
      "Succ/Fail/Skip: 318/88/94                                                                 \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.148                                   \n",
      "--> Loss: -0.7183382463054188                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1335, lambda_thesaurus: 0.2333, lambda_we: 0.6332             \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.141                                   \n",
      "--> Loss: -0.7319731330049262                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5773, lambda_thesaurus: 0.1846, lambda_we: 0.2381             \n",
      "Succ/Fail/Skip: 317/89/94                                                                 \n",
      "--> SBERT Similarity: 0.895, Contraddiction Rate: 0.151                                   \n",
      "--> Loss: -0.7160126354679802                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4552, lambda_thesaurus: 0.0279, lambda_we: 0.5169             \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.144                                   \n",
      "--> Loss: -0.7292791330049262                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.1665, lambda_thesaurus: 0.5919, lambda_we: 0.2416             \n",
      "Succ/Fail/Skip: 303/103/94                                                                \n",
      "--> SBERT Similarity: 0.89, Contraddiction Rate: 0.158                                    \n",
      "--> Loss: -0.6986410837438423                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5265, lambda_thesaurus: 0.1624, lambda_we: 0.311              \n",
      "Succ/Fail/Skip: 326/80/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.135                                   \n",
      "--> Loss: -0.736496133004926                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6057, lambda_thesaurus: 0.1475, lambda_we: 0.2468             \n",
      "Succ/Fail/Skip: 321/85/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.134                                   \n",
      "--> Loss: -0.735796078817734                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3857, lambda_thesaurus: 0.1035, lambda_we: 0.5108             \n",
      "Succ/Fail/Skip: 325/81/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.142                                  \n",
      "--> Loss: -0.7305825221674878                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2885, lambda_thesaurus: 0.2093, lambda_we: 0.5022            \n",
      "Succ/Fail/Skip: 328/78/94                                                                \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.137                                  \n",
      "--> Loss: -0.735687354679803                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5016, lambda_thesaurus: 0.2567, lambda_we: 0.2417            \n",
      "Succ/Fail/Skip: 312/94/94                                                                \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.151                                  \n",
      "--> Loss: -0.7127005812807882                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3628, lambda_thesaurus: 0.3865, lambda_we: 0.2506            \n",
      "Succ/Fail/Skip: 303/103/94                                                               \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.139                                  \n",
      "--> Loss: -0.7224390837438424                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4098, lambda_thesaurus: 0.1452, lambda_we: 0.445             \n",
      "Succ/Fail/Skip: 326/80/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.138                                  \n",
      "--> Loss: -0.7346671330049261                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8818, lambda_thesaurus: 0.0409, lambda_we: 0.0773            \n",
      "Succ/Fail/Skip: 88/318/94                                                                \n",
      "--> SBERT Similarity: 0.922, Contraddiction Rate: 0.102                                  \n",
      "--> Loss: -0.6713057536945812                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2259, lambda_thesaurus: 0.1649, lambda_we: 0.6092            \n",
      "Succ/Fail/Skip: 327/79/94                                                                \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.144                                  \n",
      "--> Loss: -0.7289157438423645                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.4631, lambda_thesaurus: 0.2273, lambda_we: 0.3096            \n",
      "Succ/Fail/Skip: 323/83/94                                                                \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.149                                  \n",
      "--> Loss: -0.7216093004926107                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.3232, lambda_thesaurus: 0.3743, lambda_we: 0.3024            \n",
      "Succ/Fail/Skip: 317/89/94                                                                \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.148                                  \n",
      "--> Loss: -0.7195496354679802                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.562, lambda_thesaurus: 0.1717, lambda_we: 0.2663             \n",
      "Succ/Fail/Skip: 323/83/94                                                                \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.142                                  \n",
      "--> Loss: -0.7287393004926108                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5162, lambda_thesaurus: 0.1467, lambda_we: 0.3371            \n",
      "Succ/Fail/Skip: 327/79/94                                                                \n",
      "--> SBERT Similarity: 0.896, Contraddiction Rate: 0.138                                  \n",
      "--> Loss: -0.7334357438423645                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6206, lambda_thesaurus: 0.1761, lambda_we: 0.2033            \n",
      "Succ/Fail/Skip: 309/97/94                                                                \n",
      "--> SBERT Similarity: 0.894, Contraddiction Rate: 0.146                                  \n",
      "--> Loss: -0.715692748768473                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.2511, lambda_thesaurus: 0.4719, lambda_we: 0.277             \n",
      "Succ/Fail/Skip: 300/106/94                                                               \n",
      "--> SBERT Similarity: 0.895, Contraddiction Rate: 0.153                                  \n",
      "--> Loss: -0.7058482512315271                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.5541, lambda_thesaurus: 0.0529, lambda_we: 0.393             \n",
      "Succ/Fail/Skip: 325/81/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.142                                  \n",
      "--> Loss: -0.7305825221674878                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7674, lambda_thesaurus: 0.0597, lambda_we: 0.173             \n",
      "Succ/Fail/Skip: 311/95/94                                                                \n",
      "--> SBERT Similarity: 0.9, Contraddiction Rate: 0.119                                    \n",
      "--> Loss: -0.7461019704433498                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9192, lambda_thesaurus: 0.0067, lambda_we: 0.0741             \n",
      "Succ/Fail/Skip: 24/382/94                                                                 \n",
      "--> SBERT Similarity: 0.956, Contraddiction Rate: 0.042                                   \n",
      "--> Loss: -0.7276706600985221                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9721, lambda_thesaurus: 0.005, lambda_we: 0.0228              \n",
      "Succ/Fail/Skip: 16/390/94                                                                 \n",
      "--> SBERT Similarity: 0.972, Contraddiction Rate: 0.062                                   \n",
      "--> Loss: -0.7196177733990147                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7821, lambda_thesaurus: 0.0553, lambda_we: 0.1625             \n",
      "Succ/Fail/Skip: 307/99/94                                                                 \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.14                                    \n",
      "--> Loss: -0.7243715270935961                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6873, lambda_thesaurus: 0.0471, lambda_we: 0.2656             \n",
      "Succ/Fail/Skip: 322/84/94                                                                 \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.14                                    \n",
      "--> Loss: -0.7317606896551725                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8, lambda_thesaurus: 0.0013, lambda_we: 0.1987                \n",
      "Succ/Fail/Skip: 313/93/94                                                                 \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.134                                   \n",
      "--> Loss: -0.7318551921182266                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8399, lambda_thesaurus: 0.0441, lambda_we: 0.116              \n",
      "Succ/Fail/Skip: 211/195/94                                                                \n",
      "--> SBERT Similarity: 0.908, Contraddiction Rate: 0.066                                   \n",
      "--> Loss: -0.7520128866995073                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.727, lambda_thesaurus: 0.0552, lambda_we: 0.2178              \n",
      "Succ/Fail/Skip: 317/89/94                                                                 \n",
      "--> SBERT Similarity: 0.897, Contraddiction Rate: 0.129                                   \n",
      "--> Loss: -0.7374446354679803                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8635, lambda_thesaurus: 0.0363, lambda_we: 0.1002             \n",
      "Succ/Fail/Skip: 139/267/94                                                                \n",
      "--> SBERT Similarity: 0.908, Contraddiction Rate: 0.065                                   \n",
      "--> Loss: -0.717452906403941                                                              \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9372, lambda_thesaurus: 0.0203, lambda_we: 0.0426             \n",
      "Succ/Fail/Skip: 18/388/94                                                                 \n",
      "--> SBERT Similarity: 0.965, Contraddiction Rate: 0.056                                   \n",
      "--> Loss: -0.7198269950738915                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9984, lambda_thesaurus: 0.0016, lambda_we: 0.0                \n",
      "Succ/Fail/Skip: 16/390/94                                                                 \n",
      "--> SBERT Similarity: 0.966, Contraddiction Rate: 0.062                                   \n",
      "--> Loss: -0.7139897733990147                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8296, lambda_thesaurus: 0.0385, lambda_we: 0.1319             \n",
      "Succ/Fail/Skip: 254/152/94                                                               \n",
      "--> SBERT Similarity: 0.902, Contraddiction Rate: 0.134                                  \n",
      "--> Loss: -0.7062551527093597                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9243, lambda_thesaurus: 0.0283, lambda_we: 0.0474            \n",
      "Succ/Fail/Skip: 20/386/94                                                                \n",
      "--> SBERT Similarity: 0.966, Contraddiction Rate: 0.05                                   \n",
      "--> Loss: -0.7275522167487685                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6909, lambda_thesaurus: 0.0356, lambda_we: 0.2735            \n",
      "Succ/Fail/Skip: 322/84/94                                                                \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.137                                  \n",
      "--> Loss: -0.7344576896551724                                                            \n",
      "\n",
      "Now tring --> lambda_mlm: 0.7485, lambda_thesaurus: 0.0394, lambda_we: 0.2122             \n",
      "Succ/Fail/Skip: 315/91/94                                                                 \n",
      "--> SBERT Similarity: 0.899, Contraddiction Rate: 0.13                                    \n",
      "--> Loss: -0.7373024137931035                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8363, lambda_thesaurus: 0.0727, lambda_we: 0.0909             \n",
      "Succ/Fail/Skip: 153/253/94                                                                \n",
      "--> SBERT Similarity: 0.898, Contraddiction Rate: 0.059                                   \n",
      "--> Loss: -0.7203874581280789                                                             \n",
      "\n",
      "Now tring --> lambda_mlm: 0.6615, lambda_thesaurus: 0.3213, lambda_we: 0.0172             \n",
      "Succ/Fail/Skip: 226/180/94                                                              \n",
      "--> SBERT Similarity: 0.881, Contraddiction Rate: 0.119                                 \n",
      "--> Loss: -0.6874910492610837                                                           \n",
      "\n",
      "Now tring --> lambda_mlm: 0.8994, lambda_thesaurus: 0.0411, lambda_we: 0.0594           \n",
      "Succ/Fail/Skip: 30/376/94                                                              \n",
      "--> SBERT Similarity: 0.959, Contraddiction Rate: 0.033                                \n",
      "--> Loss: -0.7421313251231527                                                          \n",
      "\n",
      "Now tring --> lambda_mlm: 0.9539, lambda_thesaurus: 0.0374, lambda_we: 0.0087          \n",
      "Succ/Fail/Skip: 16/390/94                                                              \n",
      "--> SBERT Similarity: 0.971, Contraddiction Rate: 0.062                                \n",
      "--> Loss: -0.7186797733990147                                                          \n",
      "\n",
      "100%|██████████| 100/100 [29:13:48<00:00, 1052.29s/trial, best loss: -0.7520128866995073]\n",
      "\n",
      "The best combination of hyperparameters is:\n",
      "lambda_mlm: 0.8399, lambda_thesaurus: 0.0441, lambda_we: 0.116\n"
     ]
    }
   ],
   "source": [
    "# define the search space across the hyperparameters\n",
    "space = hp.choice(\n",
    "    \"weights\",\n",
    "    [\n",
    "        {\n",
    "            \"w1\": hp.uniform(\"w1\", 0, 1),\n",
    "            \"w2\": hp.uniform(\"w2\", 0, 1),\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "best = fmin(\n",
    "    optimize,\n",
    "    space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    ")\n",
    "\n",
    "print(\"\\nThe best combination of hyperparameters is:\")\n",
    "w1 = best[\"w1\"]\n",
    "w2 = (1 - best[\"w1\"]) * best[\"w2\"]\n",
    "w3 = (1 - best[\"w1\"]) * (1 - best[\"w2\"])\n",
    "print(f\"lambda_mlm: {round(w1, 4)}, lambda_thesaurus: {round(w2, 4)}, lambda_we: {round(w3, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': 0.8398808190105614, 'w2': 0.27544764840994906, 'weights': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('3rdplace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74395c6fa573f9609fbdb613d31c3b5e5bf3bb88cc4a59c4ca3c6b11483c6e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
