\section{Summary of findings}\label{sec:summary-of-findings}
%**************************************************************
% A clear answer to your research question or hypothesis
% Summary of the main findings or argument
% Connections between your findings or argument to other research
% Explanation and significance of the findings
% Implications of the findings
% Limitations of the research and methodology
% Recommendations for future research


Overall, we study adversarial attacks against state-of-the-art text classification models.
In this dissertation, we propose a high-quality and effective method, called SynBA, to generate adversarial examples using a novel word transformation mechanism. It attempts to propose word perturbations that are semantically correlated to the original word and fit into the context of the sentence.

Experiment results demonstrate the strength and effectiveness of our proposed system in generating untargeted adversarial texts. 
It achieves a high semantic similarity score while maintaining a minimum perturbation and a low contradiction rate.

Using a human evaluation, we have shown that most perturbations introduced through state-of-the-art adversarial attacks do not preserve semantics. This is contrary to what is generally claimed in papers introducing these attacks. We believe the main reason for this discrepancy is that researchers working on attacks have not paid enough attention to preserving semantics because attacks with new state-of-the-art success rates are easier to publish.
Moreover, annotations are useful to validate that most of the time the adversarial examples generated by SynBA are consistent in meaning to the original texts.

The proposed contradiction rate metric turns out to be correlated with human judgement, so it is a good indicator of human prediction consistency and allows it to be assessed automatically without the need for annotations requiring significant human effort.


\subsection{Limitations}\label{subsec:limitations}
%**************************************************************
It might be possible that all the word candidates with a high \emph{thesaurus} and \emph{word embedding score} are rejected by the constraints, so the perturbed words are defined only by candidates suggested from the masked language model.
In this case, we end up again in the same situation of BERT-based attacks, where the perturbed words could be inconsistent or unrelated to the original counterpart.

Furthermore, we assessed the proposed method only on movie reviews dataset, so it is not clear if it still performs well on other datasets.
Also the target model has an important role in the process of generating adversarial examples, since according to its robustness an attack can be more or less successful. Testing only the fine-tuned version of the BERT-base model, we cannot say if the proposed method is effective also on other models.

And besides, the contradiction rate metric seems to be promising in the context of sentiment analysis, but it could be less informative for other classification tasks.