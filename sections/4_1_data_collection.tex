\section{Data collection}\label{sec:data-collection}
%**************************************************************

%**************************************************************
\subsection{Experimental setup}\label{subsec:experimental-setup}

%**************************************************************

\subsection{Datasets perturbed}\label{subsec:datasets-perturbed}
Thus the sequence length plays an important role in the high-quality perturbation process. 

%**************************************************************

\subsection{Model attacked}\label{subsec:model-attacked}

Pre-trained language models have become mainstream for many NLP tasks.
craft adversarial samples to fool the fine-tuned BERT model.
We use the BERT-base-uncased model provided by the Hugging Face Transformers (Wolf et al.,

%**************************************************************