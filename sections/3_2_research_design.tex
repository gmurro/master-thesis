\section{Research design}\label{sec:research-design}
%**************************************************************

Although adversarial attacks are a practical approach to evaluate robustness, most of them have the problem of being task-specific, not being well generalized.
Thus, this thesis is focused only on the text classification task, including sentiment analysis, topic classification, and natural language inference.

As baseline methods for adversarial attacks, we choose TextFooler \cite{journals/corr/abs-1907-11932} and BAE \cite{conf/emnlp/GargR20}.
Those are compared with the proposed method from a qualitative perspective. In addition, an efficiency assessment is carried out to measure the execution time.

\subsection{Attack category}\label{subsec:attack-category}

There are exploding combinations of the categories listed in Section \ref{subsec:taxonomy-textual-adversarial-attacks} into which our proposed attack method can fall into.
As a design choice, we defined the category of the attack in which we want to conduct our research.

Mainstream work has focused on word-level perturbation because of the large search space of substitution words and the hardness to maintain sentence semantics. 
Word-level methods usually maintain imperceptibility better than other attacks, so we also explored this category of attacks.

Differently from the baseline methods (TextFooler and BAE), our proposal attempt to generate adversarial examples in a white-box setting, which means that the attacker has access to the target model. 
This is a more realistic scenario, since the goal of the attacker is to craft adversaries with the purpose of enhancing the robustness of the target model. 
Thus, it has full access to the model and can exploit it to generate the perturbations.

The attack strategy and the adversarial goal are the same as the baselines, respectively importance-based and non-targeted attack.

