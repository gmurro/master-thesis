\section{Machine Learning hardening}\label{sec:ml-hardening}
%**************************************************************
Adversarial examples demonstrate that many modern machine learning algorithms can be broken easily in surprising ways. 
An essential purpose for generating adversarial examples for neural networks is to utilize these adversarial examples to enhance the model's robustness.

The overwhelming amount of work in the last few years for adversarial defences has given good competition to the novel adversarial attack algorithms and considerably improved the robustness of existing deep learning models. These defence mechanisms are also used as regularization techniques to avoid overfitting, and making the model more robust \cite{https://doi.org/10.48550/arxiv.2203.06414}.

%**************************************************************
\subsection{Vanilla adversarial training}\label{subsec:adversarial-training}
One of the most popular adversarial defence approach is adversarial training.
It was first introduced in the work proposed in \cite{goodfellow2014explaining}. 
It is a method of defending against adversarial attacks by introducing adversarial examples in the training data. 
The strength of adversarial examples decides the final robustness and generalization achieved by the model.

This method can be seen as a data augmentation mechanism that extends the original training set with the successfully generated adversarial examples and try to let the model see more data during the training process.
Adversarial examples need to be carefully designed when training on adversarial examples to improve the model.

Although adversarial training can effectively improve the robustness of NLP models, this approach has some problems: 
\begin{itemize}
    \item extensive adversarial examples need to be prepared in advance, resulting in a massive calculation consumption
    \item it is likely to reduce the model classification accuracy
\end{itemize}

%**************************************************************
\subsection{Attack to Training}\label{subsec:a2t}
High computational cost hinders the use of vanilla adversarial training in NLP, and it is unclear how and to what extent such training can improve an NLP model's performance.

Yoo et al. \cite{https://doi.org/10.48550/arxiv.2109.00544} propose to improve the vanilla
adversarial training in NLP with a computationally cheaper adversary, referred to as \acrfull{a2t}. 
A2T attempts to generate adversarial examples on the fly during training of the model on the training set, which is much cheaper than generating adversarial examples in advance.
This approach can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks.

The attack component in A2T is designed to be is faster than previous attacks from the literature. Previous attacks such as \cite{conf/emnlp/GargR20, journals/corr/abs-1907-11932} iteratively replace one word at a time to generate adversarial examples.
One issue with this method is that an additional
forward pass of the model must be made for each.
word to calculate its importance. For longer text inputs, this can mean that we have to make up to hundreds of forward passes to generate one adversarial example.

A2T instead determines each word's importance
using the gradient of the loss. For an input text including $n$ words: $x = (x_1, x_2, . . . , x_n)$ where each $x_i$ is a word, the importance of $x_i$ is calculated as:
\begin{equation}
    I(x_i) = \| \nabla_{e_i} L(\theta, x, y) \|_1
\end{equation}

where $e_i$ is the word embedding that corresponds to word $x_i$. For BERT and RoBERTa models where inputs are tokenized into sub-words, we calculate the importance of each word by taking the average of all sub-words constituting the word. This requires only one forward and backward
pass and saves us from having to make additional forward passes for each word.
