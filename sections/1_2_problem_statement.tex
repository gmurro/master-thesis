\section{Problem statement}\label{sec:problem-statement}
%**************************************************************

One critical issue in adversarial settings is to understand whether and to what extent a classifier may resist to specifically targeted attacks.

An adversarial example is an instance with small, intentional feature perturbations that cause a machine learning model to make a false prediction.
These carefully curated examples are correctly classified by a human observer but can fool a target model, raising serious concerns regarding the security and integrity of existing ML algorithms. 
On the other hand, it is shown that the robustness and generalization of ML models can be improved by crafting high-quality adversaries and including them in the training data.

While existing works on adversarial examples have obtained success in the image and speech domains, it is still challenging to deal with text data due to its discrete nature.

The intrinsic difference between images and textual data makes it extraordinarily difficult for
researchers to employ methods dealing with images to adversarial attacks in the NLP domain.
