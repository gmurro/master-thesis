\section{Problem statement}\label{sec:problem-statement}
%**************************************************************
While existing works on adversarial examples have obtained success in the image and speech domains (Szegedy et al. 2013; Carlini and Wagner 2018), it is still challenging to deal with text data due to its discrete nature.

These carefully curated examples are correctly classified by a human observer but can fool a target model, raising serious concerns regarding the security and integrity of existing ML algorithms. On the other hand, it is showed that robustness and generalization of ML models can be improved by crafting high-quality adversaries and including them in the training data.

The intrinsic difference between images and textual data makes it extraordinarily difficult for
researchers to employ methods dealing with images to adversarial attacks in the NLP domain.
