\section{Topic definition}\label{sec:topic-definition}
%**************************************************************

In the era of digital transformation, the amount of data produced by humans is growing exponentially. 
We are continuously exposed to an immense quantity of information, and by interacting with digital devices we are constantly generating data
in the form of text, images, videos, and audio.

This data is used to train \acrfull{ml} models able to perform tasks that were previously accomplished by humans. 
However, the security and integrity of these models are still a concern. 
In particular, the ability of ML models to generalize to unseen data is challenging. 

The mathematics behind ML models is complex, and it is difficult to understand how they make decisions. 
This lack of transparency can be exploited by Adversarial Machine Learning, a novel research area that lies at the intersection of machine learning and cybersecurity.
It refers to a class of attacks that aims to deteriorate the performance of classifiers on specific tasks.
The goal behind adversarial attacks is to circumvent existing parameters and data rules so that the ML model confuses its instructions and makes a mistake.

With machine learning rapidly becoming core to organizations' value proposition, the need for organizations to protect them is growing fast. Hence, Adversarial Machine Learning is becoming an important field in the software industry.

Gartner, a leading industry market research firm, advised that application leaders must anticipate and prepare to mitigate potential risks of data corruption, model theft, and adversarial examples.

% In this thesis, we will focus on the problem of adversarial examples, which are carefully crafted inputs that are correctly classified by a human observer but can fool a target model. 
% We will propose a novel method to generate adversarial examples for text data. We will show that our method is able to generate high-quality adversarial examples that can fool a target model. We will also show that the adversarial examples generated by our method can be used to improve the robustness and generalization of the target model.

% To understand why AI systems are vulnerable to the same weakness, we
% must briefly examine how AI algorithms, or more specifically the machine
% learning techniques they employ, “learn.” Just like the reconnaissance officers, the machine learning algorithms powering AI systems “learn” by
% extracting patterns from data. 